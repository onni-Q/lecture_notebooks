{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onni-Q/lecture_notebooks/blob/main/Week3_Lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please run the cell below to set up the environment."
      ],
      "metadata": {
        "id": "QNU7AdcgFOkJ"
      },
      "id": "QNU7AdcgFOkJ"
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep cell\n",
        "%%capture\n",
        "\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "try:\n",
        "  import palettable\n",
        "except:\n",
        "  !pip install palettable\n",
        "  import palettable\n",
        "\n",
        "\n",
        "if not(os.path.exists('snsdata.csv')):\n",
        "  print('Downloading snsdata.csv\\n')\n",
        "  !wget https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/snsdata.csv\n",
        "\n",
        "if not(os.path.exists('spotify-dataset.csv')):\n",
        "  print('Downloading spotify-dataset.csv\\n')\n",
        "  !wget https://raw.githubusercontent.com/goto4711/cdai/refs/heads/main/spotify-dataset.csv\n",
        "\n",
        "if not(os.path.exists('CDAIML.py')):\n",
        "  print('Downloading CDAIML.py\\n')\n",
        "  !wget https://raw.githubusercontent.com/goto4711/cdai/refs/heads/main/CDAIML.py\n",
        "\n",
        "\n",
        "from CDAIML import DrawNN\n",
        "from CDAIML import display_youtube"
      ],
      "metadata": {
        "id": "bUW76_Ccngfh"
      },
      "id": "bUW76_Ccngfh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "119cefbc",
      "metadata": {
        "id": "119cefbc"
      },
      "source": [
        "# Machine Learning\n",
        "\n",
        "## Supervised Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a7adadc",
      "metadata": {
        "id": "5a7adadc"
      },
      "source": [
        "Today, we will learn that machine learning is much less scary than science fiction wants us to believe. This is not because we have benevolent machines, which only want our best, but simply because these machines are quite far away from living their own life without our input, as Skynet manages in 'Terminator'. For the time being, machines still learn best when provided with human input. Furthermore, machines learn in most applications not because they want to start to understand the meaning of life and find out that humans are obstacles to true life, but because they learn to complete a particular task. Machines learn to be parts of the workbenches of digital productions.\n",
        "\n",
        "Machines still largely learn from our examples and need to be fed with large amounts of data to learn. But this makes machine learning also an ethically difficult endeavour. Machine learning demands ever more data, which requires vast amounts of energy. Most aspects of our lives are recorded in gigantic data stores that are easily accessible to machines. Governments, businesses and individuals are recording and reporting all manners of information from the monumental to the mundane. As long as the activities can be transformed into digital formats, you can be certain that somebody will record it.\n",
        "\n",
        "In such a world, machines learn by consuming data and humans continuously add new digital methods of machine learning that can exploit this data. These can be some of the simple statistical methods we have already met or more advanced ones, we will meet today. The digital methods we learn about today have in common that they aim to predict new observations from old observations. They are all empirical and predictive using models.\n",
        "\n",
        "Machine-learning algorithms are all around you. They have tried to predict the outcomes of elections and referenda, can identify spam messages, predict crime and natural disasters, target donors and voters as well as have learned how to drive cars - sometimes. They also frequently get it wrong, as many news articles will tell you. Many stories are told about the uses and abuses of machine learning. Given how much machine learning is now part of our everyday life, it is maybe surprising that there are not even more such stories.\n",
        "\n",
        "We also still lack an ethics of machine learning, which is developing so fast that it is difficult for laws and norms to stay up to date. There is, for instance, an on-going debate how biased machine learning algorithms are with regard to race and gender. Machine learning has also made it possible to identify people based on the region they live, the products they buy, etc. As a machine-learning practitioner, you are often required to exclude revealing data that is ethically problematic. But this is not an easy task, as sometimes the connections are not obvious and might only be revealed after you have trained the machine to learn.\n",
        "\n",
        "As a machine-learning practitioner, you will also quickly find out that many of the processes of machine learning repeat themselves and are made easier nowadays with powerful Python libraries, ChatGPT and Gemini. Today we start with how so-called 'supervised machine learning' relies on labelled input and output training data, whereas 'unsupervised learning' processes unlabelled or raw data but often requires extensive human interpretation efforts. In this session, you will get to know some of the most important algorithms - supervised and unsupervised - to deal with what we call structured data (https://www.ibm.com/think/topics/structured-vs-unstructured-data). Structured data looks like tables in a spreadsheet and is highly organized and easily decipherable by machine-learning algorithms.\n",
        "\n",
        "By the end of this lecture, you will understand:\n",
        "-\tThe basic principles of supervised and unsupervised machine learning.\n",
        "-\tThe typical data-science workflow: data collection, preparation, modelling and evaluation.\n",
        "-\tHow to prepare data for machine learning with normalization, handling missing values, etc..\n",
        "-\tThe concept of neural networks for classification.\n",
        "-\tHow to implement a simple neural network using scikit-learn.\n",
        "-\tThe concept of k-means clustering and how to implement it using scikit-learn and interpret basic results.\n",
        "\n",
        "This might seem like a very fast-paced session, but the principles of it should be not so hard to understand. In fact, the most important thing to understand is what the data-science workflow is and that there is this miracle Python library `scikit-learn` that does all the hard work.\n",
        "\n",
        "We start with neural networks for supervised learning and finish with clustering and unsupervised learning.\n",
        "\n",
        "Let's load today's libraries that should be familiar to you by now. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7e7cbd",
      "metadata": {
        "id": "4d7e7cbd"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083e24cf",
      "metadata": {
        "id": "083e24cf"
      },
      "source": [
        "## Sidebar: The Data Science Process\n",
        "\n",
        "### A word on our data\n",
        "\n",
        "Just like humans, machines use data to generalize. They abstract the data and develop its underlying principles, because humans tell them how. In the words of machine learning, machines form a model, which assigns meaning and represents knowledge. The model summarizes the data and makes explicit patterns among data.\n",
        "\n",
        "There are many different types of models. We have already seen some and others you will know from school. Models can be (statistical) equations, figures like graphs or trees, rules or clusters. Machines don't choose the type of models. We choose them for them when analysing the task at hand and the available data.\n",
        "\n",
        "The computer learns to fit the model to the data in a process called training. However, computational modelling does not end here. We also need to test the model in a separate testing process. The model thus does not include anything else but what can be found in the data already. It can nevertheless be interesting, as the model might surface connections that we did not recognize before. Newton discovered gravity this way by fitting a series of equations (a model) to observations of falling apples - if the myth is to be believed. Gravity was always there, but it was observed for the first time in a model. Modelling is then also never perfect. It generally involves some kind of bias or systematic error. Newton's laws of gravity are not as universal as he thought they would be. Errors like this do not have to be a bad thing, because they can lead the computer to learning a better model, correcting previous mistakes.\n",
        "\n",
        "Unfortunately, especially in our domain of social and cultural data analysis, models often fall short of desirable performance. Humans are difficult for computers and their data is very noisy. Humans are also inconsistent and report data wrongly. This means that social and cultural data includes many errors because observations have not been measured correctly or maybe they are simply impossible to measure. How do you quantify, for instance, love? It seems impossible, but online match-making agencies still make a business out of predicting love. Finally, especially in history we simply do not have data for all time periods. Often, the records have been lost. Even if we have data, it will include many missing values or will be badly captured according to diverse and sometimes contradictory standards.\n",
        "\n",
        "A final complication with data in social and cultural data analysis that has only recently emerged is the limited access we have to the data. Because it is so valuable, it is kept behind the walls of company servers or government agencies and is not shared.\n",
        "\n",
        "So, machine learning is not artificial intelligence yet, but a laborious collaboration between humans and machines that involves trying models and fighting with (bad) data. Machine learning then leads to data science, which is a process that consists of a series of repeatable steps, which we will learn about today.\n",
        "\n",
        "Schutt and O'Neil (2013), have given us an excellent overview of the art of data science.\n",
        "\n",
        "![alt text](https://github.com/goto4711/cdai/blob/main/process.png?raw=true \"The Art of Data Science \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e260cdb8",
      "metadata": {
        "id": "e260cdb8"
      },
      "source": [
        "According to the Figure, we first need to collect (raw) data in a form that we can process it. The next step explores the data and cleans it. People in data analysis like to emphasize that this is about 80% of the whole work. Then, we need a question we would like to answer with the data. This question will of course be at the beginning of our work but will likely also change after the initial exploration. Based on the question and the exploration, we start with the model and train it using a subset of the data. After training, we need to evaluate the model's performance by running a series of test predictions against test data. The result of the evaluation will then be used to improve the model's performance iteratively until we are satisfied that the model performs as best as possible, and decisions can be confidently made.\n",
        "\n",
        "Before we experience the art of machine learning and prediction, let's quickly remind ourselves of what data is in the eye of the machine. Data generally describes a series of observations, which in Python may be captured in the rows of a dataframe. Each observation is defined by its features (characteristics), which are the columns of a dataframe. If a feature represents a characteristic measured in numbers, it is unsurprisingly called numeric. For instance, the ages of a group of students is numerical. Alternatively, if a feature measures an attribute that is represented by a set of categories, the feature is called categorical or nominal. For instance, the colour codes for red, green and blue are categorical. A special case of categorical variables is called ordinal, which designates a nominal variable with categories falling in an ordered list. Movie reviews on Netflix are, for instance, ordinal, because they cover numbers from 1 to 5 but it is not clear what lies, e.g., between 2 and 3.\n",
        "\n",
        "Our friends from DataCamp have provided this nice overview:\n",
        "\n",
        "![alt text](https://github.com/goto4711/cdai/blob/main/D08t9.jpg?raw=true \"Qualitative and Quantitative Data\")\n",
        "\n",
        "Can you see how problematic certain characterizations can be? The image is taken from their tutorial at https://www.datacamp.com/tutorial/categorical-data.  \n",
        "\n",
        "Today, we focus on some very classic machine-learning approaches and start with the much larger group of supervised-learning algorithms, where an algorithm is given a set of training data and then learns a combination of features that predicts certain behaviour such as whether an earthquake will take place soon or a crime will be committed. What we are trying to predict is also called a target variable; or sometimes also label.\n",
        "\n",
        "## Predicting Taste\n",
        "\n",
        "Today, we will predict something that seems to define a human as inherently subjective. We will predict music and in particular how danceable it is. In the language of machine learning, this is a classification task. Our classification will predict whether any music from Spotify will fall into either one of two classes: good or bad danceability.\n",
        "\n",
        "We will thus solve an ancient problem of philosophy, which interrogates the aesthetic judgement (http://plato.stanford.edu/entries/aesthetic-judgment/). For the German philosopher Kant, taste judgments are universal and subjective at the same time. A key part of his Critique of Judgement, Kant demands more from taste than we are generally willing to attribute to it: 'Many things may for [a person] possess charm and agreeableness — no one cares about that; but when he puts a thing on a pedestal and calls it beautiful, he demands the same delight from others. He judges not merely for himself, but for all men, and then speaks of beauty as if it were a property of things. (…). He blames them if they judge differently, and denies them taste, which he still requires of them as something they ought to have; (…).' (http://oll.libertyfund.org/titles/kant-the-critique-of-judgement, §7). Today, we will use the machine to find out how something can be subjective and universal at the same time.\n",
        "\n",
        "### Sidebar: Feature Spaces\n",
        "\n",
        "To illustrate how machines classify, let's first go through a simplified dataset that helps us understand taste not just for music. Because we like it sweet and crunchy, we create a training dataset by tasting 15 foods and record for each of them how crunchy and how sweet they were. Both crunchy and sweet are defined as ordinal features with a range from 1 to 10. Next, we would like to map this data into a so-called 'feature space' with two axes: one for crunchiness and one for sweetness. This example is taken from the old but still excellent 'Machine Learning' (Lantz, B. (2013). Packt Publishing Ltd.), which will later on also provide us with a nice dataset to work though unsupervised learning.\n",
        "\n",
        "Lantz produced a nice visualisation of such a feature space with a few example foods:\n",
        "![alt text](https://github.com/goto4711/cdai/blob/main/lantz-1.png?raw=true \"\")\n",
        "\n",
        "Lantz notices that in this feature space 'similar types of food tend to be grouped closely together. (…), vegetables tend to be crunchy but not sweet, fruits tend to be sweet and either crunchy or not crunchy, while proteins tend to be neither crunchy nor sweet.' (p. 68). Similarity is thus based on the distance of the items in the feature space.\n",
        "\n",
        "![alt text](https://github.com/goto4711/cdai/blob/main/lantz-2.png?raw=true \"\")\n",
        "\n",
        "Next, we taste for the first time a tomato and add it to the feature space.\n",
        "\n",
        "![alt text](https://github.com/goto4711/cdai/blob/main/lantz-3.png?raw=true \"\")\n",
        "\n",
        "Based on this mapping how would we classify the tomato? Is it a vegetable or a fruit? The figure is not very conclusive, because we cannot really determine which group the tomato is closer to in the feature space. We need to find an algorithm for that.\n",
        "\n",
        "You have just learned how a machine would learn and think about the tomato as well as which decisions it would have to make to understand tomatoes. Machines learn similarities in feature spaces using distances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2de17eb",
      "metadata": {
        "id": "a2de17eb"
      },
      "source": [
        "## Machine Dancing\n",
        "\n",
        "Let's go next through our example of understanding music next and explore the individual steps of machine learning more closely. The data comes from the Kaggle copy of a famous dataset from Spotify: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset. We will talk more about Kaggle in the workshops and future lectures. For now, we have downloaded the data for your, and it's enough to know that Kaggle is a famous repository for machine learning datasets - among other things.\n",
        "\n",
        "The Spotify Tracks Dataset was created through systematic use of Spotify's Web API to collect track metadata, audio features and genre classifications across 125 different musical genres (https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset). While the exact technical implementation details weren't documented by the creator, the dataset follows standard patterns for Spotify API data collection, leveraging the platform's sophisticated audio analysis capabilities and genre classification system. The dataset includes Spotify's pre-computed audio features, which are derived through a combination of signal processing and algorithmic analysis ( https://developer.spotify.com/documentation/web-api/reference/get-audio-features). This leads to an impressive number of features for each track.\n",
        "\n",
        "\n",
        "The Spotify data consists of a structured-data file in tabular format. According to its website, these are tracks over a range of 125 different genres. Each track has features associated with it, which are the columns:\n",
        "- *track_id*: The Spotify ID for the track\n",
        "artists: The artists' names who performed the track. If there is more than one artist, they are separated by ;\n",
        "- *album_name*: The album name in which the track appears.\n",
        "- *track_name*: Name of the track.\n",
        "- *popularity*: The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by an algorithm and is based, for the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity.\n",
        "- *duration_ms*: The track length in milliseconds.\n",
        "- *explicit:* Whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown).\n",
        "- *danceability*: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
        "- *energy*: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on this scale.\n",
        "- *key*: The key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1\n",
        "- *loudness*: The overall loudness of a track in decibels (dB).\n",
        "- *mode*: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
        "- *speechiness*: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
        "- *acousticness*: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence that the track is acoustic.\n",
        "- *instrumentalness*: Predicts whether a track contains no vocals. 'Ooh' and 'aah' sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly 'vocal'. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content.\n",
        "- *liveness*: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n",
        "- *valence*: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g., happy, cheerful or euphoric), while tracks with low valence sound more negative (e.g., sad, depressed or angry).\n",
        "- *tempo*: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration\n",
        "- *time_signature*: An estimated time signature. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). The time signature ranges from 3 to 7 indicating time signatures of 3/4, to 7/4.\n",
        "- *track_genre*: The genre, in which the track belongs.\n",
        "\n",
        "As you can see, even music can be transformed into a number of measurable features. It is quite complex and you don't need to remember all the details now. BTW: This example is based on one of the previous years' data projects at the end of the semester. So, fell inspired ...\n",
        "\n",
        "The first step for us is to download the structured data so that we can work with it. Let us repeat one more time the steps in detail. Perhaps the most common data format of freely available structured data are Comma-Separated Values (CSV) files, which, as the name suggests, uses the comma as a delimiter between feature columns. CSV files can be imported to and exported from many common data repositories. To load CSV into Python, we use Pandas's `read_csv()` function. You use it by specifying a path to the file you want to import, e.g. /path/to/mydata.csv, when calling the `pd.read_csv()` function after importing pandas again. Here, we use it to load the data directly from the web. Can you see how it is done?\n",
        "\n",
        "The music data should be in the same folder/place as this notebook. So, you do not need to specify a so-called path. But generally this will not be the case. If you want to learn more about paths, you can watch this video: https://www.youtube.com/watch?v=0yIFEJ879lw."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "639220b5",
      "metadata": {
        "id": "639220b5"
      },
      "source": [
        "### Collect Data\n",
        "\n",
        "Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f9f4d79",
      "metadata": {
        "id": "0f9f4d79"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "music_df = pd.read_csv('spotify-dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a2f1a0",
      "metadata": {
        "id": "a9a2f1a0"
      },
      "source": [
        "This creates a dataframe `music_df` with all the features per music track. `read_csv()` directly accesses the dataframes from the local folder. We do this to make it easier for you to work with platforms like Google Colab.\n",
        "\n",
        "This completes our first step to do machine learning, the data acquisition/collection. As you are learning, it is fairly easy, because we reuse existing material. The data is also complete, and we do not have to take care of any missing values.\n",
        "\n",
        "Next let's move on to our objective for today. As described earlier, we want the machine to learn how to experience good and bad danceability. Let's take a first look at the dataset using Pandas's `.info()` first and then `head()`. Do you know how? You might want to ask Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac106ea7",
      "metadata": {
        "id": "ac106ea7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f02944f",
      "metadata": {
        "id": "3f02944f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "26da47f5",
      "metadata": {
        "id": "26da47f5"
      },
      "source": [
        "There is a column called `danceability`, which we will use for our classification task. We will use it as the classification 'target'. Remember: `Danceability` is an numerical feature between 0 and 1, with a value of 0.0 being least danceable and 1.0 being most danceable. Now, let's see how `danceability` values are distributed. We decide to plot the classes with Seaborn's `sns.histplot(music_df['danceability'], bins=20)`. With `bins=20`, we create 20 bins to represent the whole data."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mhaLzYag4ZB"
      },
      "id": "1mhaLzYag4ZB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fdc602d6",
      "metadata": {
        "id": "fdc602d6"
      },
      "source": [
        "To make this a classification task, we need to make this numerical distribution into one of two classes: good or bad danceability. Looking at the figure/histogram, 0.6 seems to be a good cut-off point, as it splits the data in half.\n",
        "\n",
        "A very nice function to do this is `np.where`? If you know it, you also know that it is easy with `music_df['dance_label'] = np.where(music_df['danceability'] >= DANCEABILITY_THRESHOLD, 1, 0)`: 0 stands for bad danceability, 1 for good danceability.\n",
        "\n",
        "`DANCEABILITY_THRESHOLD` is a constant we can set as we want. We set it to our cut-off of 0.6. In code, you often see constants capitalized for easy access to them. It is good practice to define them in one place to reuse them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c53135f8",
      "metadata": {
        "id": "c53135f8"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "DANCEABILITY_THRESHOLD = 0.6 # Let's say songs with danceability >= 0.6 are \"High Danceability\"\n",
        "\n",
        "music_df['dance_label'] = np.where(music_df['danceability'] >= DANCEABILITY_THRESHOLD, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e70643e9",
      "metadata": {
        "id": "e70643e9"
      },
      "source": [
        "We have created a new column `dance_label` to decide whether a song is danceability and not. Let's see how this is distributed with value_counts(). Run `music_df['dance_label'].value_counts()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a731191",
      "metadata": {
        "id": "3a731191"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As suspected, they are fairly evenly distributed and our threshold is well chosen.\n",
        "\n",
        "But these are just numbers. Let's take a look at two examples. The first one is an example for music with least danceability and the second one with most danceability.\n",
        "\n",
        "Run the next two cells. Please note that for copyright reasons you click on the image to open YouTube for this. YouTube does not allow you to run these within the notebook."
      ],
      "metadata": {
        "id": "NXugamj4kTtH"
      },
      "id": "NXugamj4kTtH"
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep cell\n",
        "\n",
        "least_danceable_example = \"7eJ3I-_8Mr8\"\n",
        "display_youtube(least_danceable_example, video_title=\"Example of least danceable music\")"
      ],
      "metadata": {
        "id": "mxZ7yjtgj6PN"
      },
      "id": "mxZ7yjtgj6PN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep cell\n",
        "\n",
        "most_danceable_example = \"zVcMi4YCLWQ\"\n",
        "display_youtube(most_danceable_example, video_title=\"Example of most danceable music\")"
      ],
      "metadata": {
        "id": "uSyk88GnkGTu"
      },
      "id": "uSyk88GnkGTu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting. I am personally not sure about these choices. But it's a matter of taste ...\n",
        "\n",
        "Please run `sns.histplot(data=music_df, x='danceability', hue='dance_label', multiple='stack', bins=20)` next to see how the new `dance_label` is distributed. `stack` stacks the two classes of `dance_label`."
      ],
      "metadata": {
        "id": "s7bFFMc2nBT5"
      },
      "id": "s7bFFMc2nBT5"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBcGTlgliD3S"
      },
      "id": "wBcGTlgliD3S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a more advanced visualisation, run the next cell. It shows the top 10 genres for danceability. The kids are on top."
      ],
      "metadata": {
        "id": "qfLJPAw6l47n"
      },
      "id": "qfLJPAw6l47n"
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep cell\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "genre_counts = music_df['track_genre'].value_counts().nlargest(10)\n",
        "\n",
        "avg_danceability_per_genre = music_df.groupby('track_genre')['danceability'].mean().sort_values(ascending=False).nlargest(10)\n",
        "sns.barplot(x=avg_danceability_per_genre.values, y=avg_danceability_per_genre.index, hue = avg_danceability_per_genre.values, palette='magma', orient='h')\n",
        "plt.title(f'Average Danceability for Top Genres')\n",
        "plt.xlabel('Average Danceability Score')\n",
        "plt.ylabel('Genre')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sW2jDlpvlHNR"
      },
      "id": "sW2jDlpvlHNR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fa531577",
      "metadata": {
        "id": "fa531577"
      },
      "source": [
        "So, kids music is the most danceable? This seeems strange. In the workshop, we investigate these kinds of questions with machine learning.\n",
        "\n",
        "It's time to prepare our data for its machine-learning adventures but first try to tell Gemini to load the Spotify dataset from https://raw.githubusercontent.com/goto4711/cdai/refs/heads/main/spotify-dataset.csv, create the dance_label column with a 0.6 threshold."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zd8XxM1BXNjC"
      },
      "id": "zd8XxM1BXNjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "08d41279",
      "metadata": {
        "id": "08d41279"
      },
      "source": [
        "### Prepare Data\n",
        "\n",
        "The next step in machine learning is very important for many algorithms based on feature spaces. We need to standardize the features, as the distances in the space are dependent on how the features are measured. In particular, if certain features have much larger values than others, the distance measurements will be strongly dominated by the larger values. This wasn't a problem for us before with the food data, as both sweetness and crunchiness were measured on a scale from 1 to 10. But suppose we added another measure on a scale from 1 to 1,000,000. This measure would dwarf the contribution of the other scales. The distances in the feature space would get out of scale.\n",
        "\n",
        "We only need to normalize numeric data. Looking back at the results from `music_df.info()`, several columns/features are numeric. Next, we define a function to normalise these so that they are all on a scale between 0 and 1. We use the so-called min-max normalisation. Consider an example, where the the `popularity` of a track is 50, while we want to transform this to the range 0 to 1. So first we find the maximum value of `popularity`, which might be in this example 100 and the minimum value of `popularity`, say 20, then the new scaled value for will be: (50-20)/(100-20)=0.375. Can you see why this value is guaranteed to be between 0 and 1?\n",
        "\n",
        "Lets define a function that takes care of the normalization for us. You hopefully remember how you can define your functions in Python? Anyway, just look at the next cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0cfd2cf",
      "metadata": {
        "id": "f0cfd2cf"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "def normalize(x):\n",
        "    return ((x - x.min()) / (x.max() - x.min()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc491d1f",
      "metadata": {
        "id": "bc491d1f"
      },
      "source": [
        "Now, we want to select a number of `feature_cols`. These are the features we want investigate to see how they influence the danceability. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep cell\n",
        "\n",
        "feature_cols = [\n",
        "    'duration_ms', 'popularity', 'energy', 'key', # 'danceability' removed\n",
        "    'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
        "    'liveness', 'tempo', 'time_signature', 'valence'\n",
        "]\n",
        "\n",
        "music_features_df = music_df[feature_cols].copy()"
      ],
      "metadata": {
        "id": "w79GEDn8jfRI"
      },
      "id": "w79GEDn8jfRI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0ed676bc",
      "metadata": {
        "id": "0ed676bc"
      },
      "source": [
        "This has also created `music_features_df`.\n",
        "\n",
        "Apply is a powerful function in Pandas that let's you apply a function across several columns. Run to apply `normalize`:\n",
        "\n",
        "```\n",
        "music_features_normalized_df = music_features_df.apply(normalize)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ac30b6",
      "metadata": {
        "id": "c4ac30b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "60c8f162",
      "metadata": {
        "id": "60c8f162"
      },
      "source": [
        "Finally, let’s add the `dance_label` column to the new normalized dataframe. This time, it only contains good and bad danceability labels. Type in:\n",
        "```\n",
        "music_features_normalized_df['dance_label'] = music_df['dance_label']\n",
        "music_features_normalized_df.head()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48761834",
      "metadata": {
        "id": "48761834"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you find a prompt for Gemini to recreate these steps? Type in \"Normalise all numeric columns of music_df with min max but keep the dance-label column unchanged\"."
      ],
      "metadata": {
        "id": "eP4QAc2mUXSe"
      },
      "id": "eP4QAc2mUXSe"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NjbvtVhuUfCC"
      },
      "id": "NjbvtVhuUfCC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aaadaf50",
      "metadata": {
        "id": "aaadaf50"
      },
      "source": [
        "We are now satisfied with the data, finished our cleaning and all other preparations. We can start the modelling process in order to predict how music will be danceable. If you run this on your own machine, you might want to first save the data after all the hard work. In Pandas, this means we pickle it: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html. Run `music_features_normalized_df.to_pickle('music_features_normalized_df.pkl')`.\n",
        "\n",
        "You can also ignore this step, as the example is simple and runs quickly when you repeat it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4157d83",
      "metadata": {
        "id": "b4157d83"
      },
      "outputs": [],
      "source": [
        "#music_features_normalized_df.to_pickle('music_features_normalized_df.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5da9501",
      "metadata": {
        "id": "e5da9501"
      },
      "source": [
        "### Define the Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37c7780",
      "metadata": {
        "id": "a37c7780"
      },
      "source": [
        "From `info()` and `value_counts()`, we know that we have 114,000 music danceability observations with 61,960 labelled bad and 52,040 labelled good.\n",
        "\n",
        "Because we aim to predict new things, our next step should be to find out about things we do not already know and how the model would be able to predict unknown data. If we had access to more music data, we could collect it and apply our model to these unknown observations and see how well the predictions compare to new music. But we cannot know about data we do not have. So, we simulate such a scenario by dividing our data into a training dataset that will be used to build the model and a test dataset. We will use the test dataset to simulate the prediction with new data and find out how well our model behaves on unknown data. This is the standard evaluation way of machine learning.\n",
        "\n",
        "We will use 80% of our data for the training and 20% for testing. First, we randomly shuffle the data to ensure that all qualities are evenly distributed in both training and test data. Pandas has a function for that called sample. Create the training data with `train_set = music_features_normalized_df.sample(frac=0.80)`. `frac` is the fraction of data to be included in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf02bc9",
      "metadata": {
        "id": "faf02bc9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e7e7ca95",
      "metadata": {
        "id": "e7e7ca95"
      },
      "source": [
        "Check `info()` on `train_set` to see the changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1675a4cf",
      "metadata": {
        "id": "1675a4cf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2d06626a",
      "metadata": {
        "id": "2d06626a"
      },
      "source": [
        "We also need a test dataset. This will be simply the rest. So, we 'drop' everything from `music_features_normalized_df` that is included in the training dataset, i.e. it is in the index. You could run:\n",
        "```\n",
        "test_set = music_features_normalized_df.drop(train_set.index)\n",
        "test_set.info()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae2ad22a",
      "metadata": {
        "id": "ae2ad22a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8e6452fa",
      "metadata": {
        "id": "8e6452fa"
      },
      "source": [
        "This is fairly complicated, you can also do this much easier with `train_test_split`: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html. Run the cell below to select a `test_size` of 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff3ed6d",
      "metadata": {
        "id": "5ff3ed6d"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(music_features_normalized_df, test_size=0.20)\n",
        "test_set.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b63f491",
      "metadata": {
        "id": "6b63f491"
      },
      "source": [
        "Please note that the manual and sklearn's `test_set` datasets will generally be different from each other, as they are each based on a random selection. But they have the same size. Check out https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas for the many ways to generate test and training data.\n",
        "\n",
        "We are ready to model and because things are looking good, we go directly to one the most advanced machine-learning techniques that uses the human brain as an inspiration. Neural Networks have become synonymous with the recent success of artificial intelligence. After this, you are part of its elite developers' group."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f854374b",
      "metadata": {
        "id": "f854374b"
      },
      "source": [
        "## Modelling  \n",
        "\n",
        "### Sidebar: Neural Networks\n",
        "\n",
        "With the training data, we are ready to start learning a model for experiencing music. We will work with the best that current machine learning has to offer. We employ the help of neural networks, machines assembled in similar ways to the hundreds, thousands or millions of brain cells. Kant would be proud of us - maybe.\n",
        "\n",
        "![alt text](https://github.com/goto4711/cdai/blob/main/neural-networks-1.png?raw=true \"\")\n",
        "\n",
        "Each neuron is made up of a cell body with a number of connections coming off it. These are numerous dendrites (carrying information toward the cell body) and a single axon (carrying information away). But computers are not alive. They are mechanical boxes and made not of the complex chains of brain cells, which are densely interconnected - each one connected to perhaps 10,000 other brain cells. Computers are designed to store lots of data and rearrange that - as we have done many times - and need instructions for that. To the day, we do not fully understand how brains learn. They can spontaneously put information together in astounding new ways and forge new connections. No computer currently comes close to that.\n",
        "\n",
        "The basic idea behind a neural network is to simulate those densely interconnected brain cells inside a computer so you can get it to learn things, recognize patterns and make decisions. Neural networks learn to improve their own analysis of the data. But neural networks remain mathematical equations and mean nothing to the computers themselves - unlike our brain activities. They are still just highly interconnected numbers in boxes who constantly change.\n",
        "\n",
        "A typical neural network has anything from a few dozen to hundreds, thousands, or even millions of artificial neurons called units arranged in a series of layers, each of which connects to the layers on either side. Some of them are input units. These will be defined by the data for each feature in each observation. Each feature forms one input unit. Neural networks also have an output layer that responds to the information that is learned. In our case, these are the quality judgments we make with regard to the music.\n",
        "\n",
        "![alt text](https://github.com/goto4711/cdai/blob/main/neural-networks-2.png?raw=true \"\")\n",
        "\n",
        "In-between the input units and output units are one or more layers of hidden units, which together form the majority of this artificial brain. The connections between one unit and another one are represented by a number called a weight, which can be either positive (if one unit excites another) or negative (if one unit suppresses or inhibits another). The higher the weight, the more influence one unit has on another. Inputs are fed in from the left, activate the hidden units in the middle and feed out outputs from the right.\n",
        "\n",
        "Information flows backwards from the output units, too. For a neural network to learn, there has to be an element of feedback involved - just like we humans learn. With feedback, we compare what we tried to achieve with what we actually achieved and adjust our behaviour accordingly. Neural networks learn things in exactly the same way with a feedback process called backpropagation. Because we know from the training data the output we should get to, we can compare it with the calculated values and modify the connections in the network to get closer to the expected outcomes, working from the output units through the hidden units to the input units. Over time, this backpropagation causes the network to learn until a stable state is achieved. In our case, the network will learn how we experience music."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e31b1cc0",
      "metadata": {
        "id": "e31b1cc0"
      },
      "source": [
        "### Doing the Modelling\n",
        "\n",
        "Fortunately for us, we do not have to implement neural networks by ourselves but can rely on many existing algorithms in Python.\n",
        "\n",
        "One of the most common machine learning libraries in Python used is scikit-learn: https://scikit-learn.org/, which we have already met for the train-test split. We will use scikit's `MLP-classifier`, a Multi-layer Perceptron classifier. Perceptrons are an older name for the nodes in a neural network we just discussed and are these little mathematical entities that are supposed to simulate the human brain cells: https://en.wikipedia.org/wiki/Perceptron. In `MLP-classifier`, they are multi-layered to form very complex networks. In the Figure above we have 2 hidden layers, which is why this is a Multi-layer Perceptron classifier.\n",
        "\n",
        "Run the next cell to load the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55167bb6",
      "metadata": {
        "id": "55167bb6"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we start from what Gemini already knows.\n",
        "\n",
        "Can you ask Gemini to give you an example of a sklearn MLPClassifier to predict two different qualitative outcomes?"
      ],
      "metadata": {
        "id": "2pIp_4O5Wd7t"
      },
      "id": "2pIp_4O5Wd7t"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdHs7dRnXBbj"
      },
      "id": "cdHs7dRnXBbj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ff544852",
      "metadata": {
        "id": "ff544852"
      },
      "source": [
        "Interesting. For me, the example was good but not great. Let's do it ourselves and explain some of the concepts in Gemini's answer.\n",
        "\n",
        "We first divide our train and test set into predictor and target columns, which is our case all the columns vs the target `dance_label` column.  The next cell is very typical to all machine-learning work. For each of the two training and test datasets, it creates the part that is everything except the target variable (X) and the target variable (y). Run:\n",
        "\n",
        "```\n",
        "X_train = train_set.loc[:, train_set.columns != 'dance_label'].values\n",
        "y_train = train_set['dance_label'].values\n",
        "\n",
        "X_test = test_set.loc[:, test_set.columns != 'dance_label'].values\n",
        "y_test = test_set['dance_label'].values\n",
        "```\n",
        "\n",
        "Remember that we need arrays as inputs to `sklearn`. `values()` transforms a Pandas series into an array. The rest of the syntax is hopefully clear?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0703a600",
      "metadata": {
        "id": "0703a600"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5ff6c18e",
      "metadata": {
        "id": "5ff6c18e"
      },
      "source": [
        "And then we build the classifier. https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/ has nice instructions. Type in `model = MLPClassifier(hidden_layer_sizes=(20,10), max_iter=300)`, which defines an MLP with two hidden layers. The first one has 20 and the second 10 hidden nodes. max_iter=300 tells the modelling to stop after 300 iterations. This means after 300 forward and backward passes. These numbers  were chosen as a starting point and often require experimentation. They are called 'hyper-parameters' in machine learning: https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f7619ce",
      "metadata": {
        "id": "4f7619ce"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fa1f5c83",
      "metadata": {
        "id": "fa1f5c83"
      },
      "source": [
        "Hyperparameters are settings defined before training a machine-learning model. They control how the model learns, such as the model complexity - in our example the number of layers (2) with their sizes (20, 10). Unlike parameters learned from data, hyperparameters are manually set. Choosing the right hyperparameters is essential, as poor choices can cause lots of different problems. Check out this concise explanation: https://www.geeksforgeeks.org/machine-learning/hyperparameter-tuning/.\n",
        "\n",
        "In the Python machine learning pipeline, we now need to 'fit' the model with the training data.\n",
        "\n",
        "Run `model.fit(X_train, Y_train)` to do so. This can take a couple of minutes, as the dataset is not small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b36f4e",
      "metadata": {
        "id": "23b36f4e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "84be4a71",
      "metadata": {
        "id": "84be4a71"
      },
      "source": [
        "Run the next two cells to get an insight into how the model is constituted. The first cell prints out some of the weights attached to the neuron links and the second visualises the whole network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b30d79d",
      "metadata": {
        "id": "3b30d79d"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "print(\"Weights between input and first hidden layer:\")\n",
        "print(model.coefs_[0][:3])\n",
        "print(\"\\nWeights between first hidden and second hidden layer:\")\n",
        "print(model.coefs_[1][:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1318d44b",
      "metadata": {
        "id": "1318d44b"
      },
      "source": [
        "The next cell makes use of a little tool by http://www.jzliu.net/blog/simple-python-library-visualize-neural-network/ to visualise the connections. We have added a few improvements.\n",
        "\n",
        "Admire the complexity we generate with such a small network."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep cell\n",
        "\n",
        "network_structure = [13, 20, 10, 1]\n",
        "weights = model.coefs_\n",
        "\n",
        "vis = DrawNN(\n",
        "    network_structure,\n",
        "    weights_list=weights,\n",
        "    title=f\"Large Network {network_structure}\",\n",
        "    max_linewidth=2.5,\n",
        "    linewidth_base=0.1,\n",
        "    linewidth_scale_tier1=3.0,\n",
        "    linewidth_scale_tier2=6.0,\n",
        "    weight_text_show_threshold=0.7 # Show fewer weight texts for clarity\n",
        ")\n",
        "vis.draw()\n"
      ],
      "metadata": {
        "id": "1grxzM89mY3f"
      },
      "id": "1grxzM89mY3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "454c3bc0",
      "metadata": {
        "id": "454c3bc0"
      },
      "source": [
        "The blue connections are negative weights and the orange ones are positive.\n",
        "\n",
        "### Predicting\n",
        "\n",
        "Next, we can start predicting unknown behaviour, which - as said - we simulate with the test dataset. Run `y_pred = model.predict(X_test)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460de59b",
      "metadata": {
        "id": "460de59b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "eacfb5bb",
      "metadata": {
        "id": "eacfb5bb"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef204a2a",
      "metadata": {
        "id": "ef204a2a"
      },
      "source": [
        "Let's check out the details of our predictions and evaluate our results. To this end, we compare the predictions with test data using sklearn's confusion matrix function which prints the true positives, false negatives, false positives and true negatives . Don't know what those are? Check out: https://en.wikipedia.org/wiki/False_positives_and_false_negatives.\n",
        "\n",
        "Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2255e56e",
      "metadata": {
        "id": "2255e56e"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_pred, y_test)\n",
        "\n",
        "cm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also ask Gemini to explain the code. Prompt it with 'Please explain what the previous code cell does.'"
      ],
      "metadata": {
        "id": "_tqquVzdYM57"
      },
      "id": "_tqquVzdYM57"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1QJKO6SdYZ-e"
      },
      "id": "1QJKO6SdYZ-e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "723745ee",
      "metadata": {
        "id": "723745ee"
      },
      "source": [
        "Check https://en.wikipedia.org/wiki/Confusion_matrix for how to read this matrix but also don't worry, we will make a nicer representation. For the moment, we are only interested in the overall performance by looking at the accuracy of our prediction.\n",
        "\n",
        "Accuracy is defined as the number of times our predictions have been correct compared to the overall number of predictions. So, we take all cases where the predictions have been right in the above table (bad-bad and good-good) and compare these with the overall number of observations in the test data or len(test_set). Please replace in the calculation below the numbers you have got."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9575aa6",
      "metadata": {
        "id": "c9575aa6"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "good_good = 10490\n",
        "bad_bad = 7320\n",
        "\n",
        "(good_good + bad_bad) / len(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f6f986",
      "metadata": {
        "id": "e6f6f986"
      },
      "source": [
        "In my case, ~78% of our predictions are correct. Please, note that the exact number can be either a bit higher or lower depending on the random composition of the test and training datasets.\n",
        "\n",
        "Not bad, especially considering that most music experts would probably not be able to agree to such a degree. However, we would of course like to improve on our predictions. So, let's investigate supervised machine learning further in the workshop.\n",
        "\n",
        "But first let's make our confusion matrix a bit nicer. Run the next cell for a visualisation of the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c805ca",
      "metadata": {
        "id": "68c805ca"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_).plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcc47e6-a0b6-4012-a5c3-275fbf08b256",
      "metadata": {
        "id": "cbcc47e6-a0b6-4012-a5c3-275fbf08b256"
      },
      "source": [
        "That was it for modelling with supervised learning. It might seem complicated at first but it is really not, as the same steps are repeated again and again: from getting data, to cleaning data, to creating training and test data to finally running a model and evaluating it.\n",
        "\n",
        "In the second part of today's lecture, we concentrate on clustering and unsupervised learning. We introduce a standard, old but still very effective method called k-means."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607f8726",
      "metadata": {
        "id": "607f8726"
      },
      "source": [
        "# Unsupervised Machine Learning\n",
        "\n",
        "## Our Data: Social Networking Communities\n",
        "\n",
        "A popular application of clustering is detecting communities in social relationships.  Next we go through an example and dataset in Brett Lantz's excellent book on Machine Learning (Lantz, B. (2013). Packt Publishing Ltd.). The dataset is discussed on pp. 279. It covers the relationships in a Social Networking Service (SNS). While this is a very early SNS dataset, the data is freely available and offers similar kind of insights you can gain from my recent examples. https://github.com/stedy/Machine-Learning-with-R-datasets/tree/master has collected all the datasets from the book.\n",
        "\n",
        "Lantz explains that the dataset was compiled for sociological research on teenage identities at Notre Dame University. It represents a random sample of 30,000 US high school students who had profiles on a well-known SNS in 2006. At the time the data was collected, the SNS was a popular web destination for U.S. teenagers. Therefore, it is reasonable to assume that the profiles represent a fairly wide cross-section of US-American teenagers in 2006. The data was sampled evenly across four high school graduation years (2006 through 2009) representing the senior, junior, second-year and freshman classes at the time of data collection. Then, the full texts of the SNS profiles were downloaded. Each teen's gender, age and number of SNS friends was recorded.\n",
        "\n",
        "A text-mining tool was used to divide the remaining SNS page content into words. We will learn later in this course, how this can be done. From the top 500 words appearing across all pages, 36 words were chosen to represent five categories of interests, namely extracurricular activities, fashion, religion, romance and antisocial behaviour. The 36 words include terms such as football, sexy, kissed, bible, shopping, death, drugs, etc. The final dataset indicates, for each person, how many times each word appeared in the person's SNS profile.\n",
        "\n",
        "First, we load the relevant packages and the dataset from https://github.com/stedy/Machine-Learning-with-R-datasets/tree/master. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f2a5ae2",
      "metadata": {
        "id": "6f2a5ae2"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "teens = pd.read_csv(\"snsdata.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2caee423",
      "metadata": {
        "id": "2caee423"
      },
      "source": [
        "Print out the first couple of rows from the teens dataset. You know how ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc45cb5e",
      "metadata": {
        "id": "bc45cb5e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "17062185",
      "metadata": {
        "id": "17062185"
      },
      "source": [
        "The `teens` dataframe is now loaded into your environment. Take a close look and make sure you understand how it is produced.\n",
        "\n",
        "We can use the `info()` method to output some general information about the dataframe. Run `teens.info()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c50ea9c",
      "metadata": {
        "strip.white": true,
        "id": "9c50ea9c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "caa09bc4-945a-4821-9c78-48a65545f3db",
      "metadata": {
        "id": "caa09bc4-945a-4821-9c78-48a65545f3db"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "Part of this lesson is centered on the issue of looking into real-life data on digital society. A common problem is that observations/records are missing in such data, which is indicated by the NaN value in Python.\n",
        "\n",
        "In the `info()` printout, you can also see that the non-null count is lower for those columns that contain NaN values. Gender is one of them.\n",
        "\n",
        "Let's go through cleaning data, one step a time with the teens data. We start with the gender troubles.\n",
        "\n",
        "Print out the absolute non-null value counts for gender as well as the relative ones with:\n",
        "```\n",
        "print(teens['gender'].value_counts())\n",
        "print(teens['gender'].value_counts(normalize=True))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2e350a",
      "metadata": {
        "strip.white": true,
        "id": "3f2e350a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "adf00b23",
      "metadata": {
        "id": "adf00b23"
      },
      "source": [
        "With dropna set to False in `value_counts()`, we can also see NaN index values. Try that ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05047aaf",
      "metadata": {
        "id": "05047aaf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask Gemini how to replace the NaN values in teens['gender']."
      ],
      "metadata": {
        "id": "g600JVz6wgoe"
      },
      "id": "g600JVz6wgoe"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5TEbC6IwrKe"
      },
      "id": "B5TEbC6IwrKe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e9022978",
      "metadata": {
        "id": "e9022978"
      },
      "source": [
        "That gives us some interesting options, which we will explore later on in more detail.\n",
        "\n",
        "But missing values are not our only problem. At least as common are misreported observations in real-life data. As an example, let's look at the at the age-distribution of the teens' age. You can do this in several ways but you should always print out maximum and minimum values. Run: `teens['age'].describe()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9519a078",
      "metadata": {
        "strip.white": true,
        "id": "9519a078"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4d14e0a3",
      "metadata": {
        "id": "4d14e0a3"
      },
      "source": [
        "There are quite a few strange records here. Teens can have a minimum age of less than 4 and a maximum age of over 100! These cannot be considered teenagers.\n",
        "\n",
        "As a rule of thumb, let's assume teenagers are between 13 and 19 years old. Let's mark all other teens' age entries as invalid. We say that invalid entries should have a NaN value. Set this, please, by typing in `teens.loc[(teens['age'] < 13) | (teens['age'] >= 20), 'age'] = np.nan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff837fd",
      "metadata": {
        "strip.white": true,
        "id": "5ff837fd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5b12a9f7",
      "metadata": {
        "id": "5b12a9f7"
      },
      "source": [
        "The next step in our data-cleaning process is to replace NaN values. One common approach is to simply remove all rows/observations, for which we have null entries. But in this example, we would lose too many rows from `teens` with such a brute-force approach. So, let's try and fill the null values with estimated values.\n",
        "\n",
        "Let's start with the gender and replace null values by creating new columns for male and females. Please note that many datasets assume a binary division of gender, which is wrong. But we are only practicing here.\n",
        "\n",
        "To this end, we first create a new column to record all the female teenagers. Create a new column 'female' that is set to 1 if the teenager is female and 0 otherwise. Run `teens.loc[(teens['gender'] == 'F') & (teens['gender'].notna()), 'female'] = 1` to set the female column to 1 for females. `notna()` selects all rows that are not NaN. Pandas has many such functions, which you do not all need to remember. This is what you have your favourite chatbot for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba62159",
      "metadata": {
        "strip.white": true,
        "id": "3ba62159"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "00645ffd",
      "metadata": {
        "id": "00645ffd"
      },
      "source": [
        "Can you set the female column to 0 for males? You just need to change one letter ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7cb1b68",
      "metadata": {
        "id": "f7cb1b68"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "43390a81",
      "metadata": {
        "id": "43390a81"
      },
      "source": [
        "Next we will create another column for the null values in gender we want to call no_gender. Set this to 1 if no gender is recorded and otherwise to 0.\n",
        "\n",
        "This process is called dummy-coding and is typical to machine learning: https://en.wikipedia.org/wiki/Dummy_variable_(statistics). A dummy variable is a numerical variable used in the analysis to represent subgroups - in our case males, females and others. In research design, a dummy variable is often used to distinguish different groups to address them differently. By creating a separate column per gender entry, we can compute clusters for separate gender communities.\n",
        "\n",
        "Check out dummy-coding on the web. Can you find easier ways to do this in Pandas? Let's ask Gemini \"What is dummy coding and how do I dummy-code teens['gender']\".\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WXsblxI3KHq"
      },
      "id": "7WXsblxI3KHq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If `pd.get_dummies(teens, columns=['gender'])` is run, it will create `gender_F`, `gender_M` (and potentially `gender_NaN` if `dummy_na=True`). The manual steps create female (1 for F, 0 for M) and no_gender. The approaches are thus slightly different but achieve similar goals. The manual approach requires filling NaNs in the 'female' column for those that are 'no_gender'.\n",
        "\n",
        "We continue with our own manual approach and run:\n",
        "```\n",
        "teens.loc[teens['gender'].notna(), 'no_gender'] = 0\n",
        "teens.loc[teens['gender'].isna(), 'no_gender'] = 1\n",
        "```"
      ],
      "metadata": {
        "id": "ZZn4b-Ag3EgF"
      },
      "id": "ZZn4b-Ag3EgF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27751ab",
      "metadata": {
        "strip.white": true,
        "id": "d27751ab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a6bd616f",
      "metadata": {
        "id": "a6bd616f"
      },
      "source": [
        "After this, we have the original column, a new column called female, which contains information about whether the teen is female or not (male) and a new column with information about whether the gender value is missing. Using this column we could, for instance, check with clustering whether certain communities have a tendency not to record their gender values. How?\n",
        "\n",
        "Check out the changes with `teens.head(20)`. You have to scroll all the way to the right to find the new columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19bd88a4",
      "metadata": {
        "id": "19bd88a4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f0a0f137",
      "metadata": {
        "id": "f0a0f137"
      },
      "source": [
        "It's very easy now to calculate the number of teenagers where we do not have gender entries for. How? Remember `sum()`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14986eaa",
      "metadata": {
        "strip.white": true,
        "id": "14986eaa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2f3a0f47",
      "metadata": {
        "id": "2f3a0f47"
      },
      "source": [
        "Did you find that there are 2724?\n",
        "\n",
        "The age column is next. Can you find the average age and take care that null values are discounted?\n",
        "\n",
        "Tip: run Pandas' `mean()` and set `skipna = True` to skip NaN values: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44021755",
      "metadata": {
        "strip.white": true,
        "id": "44021755"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5c257e1b",
      "metadata": {
        "id": "5c257e1b"
      },
      "source": [
        "What would happen if you set skipna to False? Try in the above cell.\n",
        "\n",
        "A good strategy to overwrite missing age values would be to use the average age value and assign it to all of the missing ones. This process is called mean-imputation and is employed frequently. Pandas has some real strengths here. Check out https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html.\n",
        "\n",
        "Pandas makes your life very easy with its `fillna` function. Run the following cell. BTW, Gemini had given me a similar solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c59e02e",
      "metadata": {
        "id": "8c59e02e"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "teens['age'].fillna(teens['age'].mean()).quantile([.25, .5, .75])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56dab263",
      "metadata": {
        "id": "56dab263"
      },
      "source": [
        "Let's further improve this with some good old-fashioned human intelligence and beat Gemini.\n",
        "\n",
        "We feel confident that we can do better than just the mean, because we know the graduation year, too. This is the year our teens are supposed to graduate. It seems a reasonable assumption that those teenagers with an earlier graduation year should be older than those with a later graduation.\n",
        "\n",
        "We can easily find this out by running the mean function for each graduation year group separately. Type in `teens[['gradyear', 'age']].groupby(['gradyear']).mean()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711e0c1d",
      "metadata": {
        "strip.white": true,
        "id": "711e0c1d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d56093f1",
      "metadata": {
        "id": "d56093f1"
      },
      "source": [
        "Let's take a moment and look at `groupby()` once more, as it is essential knowledge to deal with Pandas: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html. It is at the heart of the split-apply-combine paradigm that will keep us often busy: https://pandas.pydata.org/docs/user_guide/groupby.html. Take a close read and you will see that we  cover all its elements throughout the session today. Groupby allows you to to 'split' data into distinct groups. This is often done with the intention to then 'apply' functions afterwards like in our case `mean()`. This works amazingly well but requires lots of practice in my experience. This will come to you but for now let's move on.\n",
        "\n",
        "According to our last output, our suspicion has proven right. There is a significant difference in the average ages depending on the graduation year. Let's use this knowledge and update missing values in the age group depending on the graduation year. To this end, you actually have to do a lot of Pandas labour, which demonstrates that 80% of the work of a data analyst lies in working with data. But I am sure you know this by now.\n",
        "\n",
        "You can, e.g., proceed with the following strategy: Create first a temporary dataset with the results from the above `group_by`. Then merge this new dataset with teens and replace the null values of `teens['age']` with the ones from the temporary dataset.\n",
        "\n",
        "Create the temporary dataset first with `ave_age = teens[['gradyear', 'age']].groupby(['gradyear'], as_index=False).mean()`. Also print out `ave_age`. It should be a data frame of three rows.\n",
        "\n",
        "Wondering about `as_index=False`? Check out https://pandas.pydata.org/docs/user_guide/groupby.html. We simply do not want to create a new index from the groupby argument gradyear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38dfb4ee",
      "metadata": {
        "strip.white": true,
        "id": "38dfb4ee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f0327af1",
      "metadata": {
        "id": "f0327af1"
      },
      "source": [
        "Update the teens age columns but make sure that in the end you have not added additional columns. First you need to merge `teens` and `ave_age` on `gradyear`. Run `teens = pd.merge(teens, ave_age, on=['gradyear'])`. Then also print out the first few columns of teens.\n",
        "\n",
        "Merge is another powerful command to learn about. It's part of the merge, join, concatenate and compare group of commands; some of which we have already met: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html. If you happen to know database languages, you will know everything about it. `merge()` combines two dataframes on common columns - in our case `gradyear`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f305941",
      "metadata": {
        "id": "3f305941"
      },
      "outputs": [],
      "source": [
        "teens = pd.merge(teens, ave_age, on=['gradyear'])\n",
        "teens.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d7ecf75",
      "metadata": {
        "id": "6d7ecf75"
      },
      "source": [
        "With some scrolling right, you should now see two age columns. age_x is the original one from teens, while age_y is the estimated value based on the gradyear. Now, we want to replace `age_x` (the original value) with `age_y` if `age_x` is NaN. Run `teens.loc[(teens['age_x'].isna()), 'age_x'] = teens['age_y']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0fd364",
      "metadata": {
        "id": "8f0fd364"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "18bb6cae",
      "metadata": {
        "id": "18bb6cae"
      },
      "source": [
        "Now, we only need to so some cleaning up. Give `age_x` its old name age back and drop `age_y`, which we don't need anymore. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b8c9f9",
      "metadata": {
        "id": "c8b8c9f9"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "teens.rename(columns={'age_x': 'age'}, inplace=True)\n",
        "teens.drop('age_y', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b087547d",
      "metadata": {
        "id": "b087547d"
      },
      "source": [
        "Use `teens.info()` to see that the age column does not contain NaNs anymore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "562d0bca",
      "metadata": {
        "id": "562d0bca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f7453237",
      "metadata": {
        "id": "f7453237"
      },
      "source": [
        "Success!\n",
        "\n",
        "Check out new `ave_age` with `teens['age'].mean()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf00cbe",
      "metadata": {
        "strip.white": true,
        "id": "bcf00cbe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "73f94179",
      "metadata": {
        "id": "73f94179"
      },
      "source": [
        "This has all been quite advanced stuff but as long as you remember the kind of steps we have taken you should be able to impute one column's missing values by using another column as a reference. In our case, we use our knowledge that age is dependent on `gradyear` to find the missing values. Please, take some time to review the steps.\n",
        "\n",
        "Let's also ask Gemini for an example of imputation in Pandas."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Give me an example of imputation in Pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Sample DataFrame with missing values\n",
        "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
        "                   'B': [5, np.nan, np.nan, 8],\n",
        "                   'C': [9, 10, 11, 12]})\n",
        "\n",
        "# Impute missing values in column 'B' with the mean\n",
        "df['B'].fillna(df['B'].mean(), inplace=True)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "kstbwU2G4j6S"
      },
      "id": "kstbwU2G4j6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at our hard work and the resulting `age` column with `teens['age'].describe()`."
      ],
      "metadata": {
        "id": "eB-Cg6gg43rV"
      },
      "id": "eB-Cg6gg43rV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d98bf60",
      "metadata": {
        "strip.white": true,
        "id": "6d98bf60"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8dd8fbd3",
      "metadata": {
        "id": "8dd8fbd3"
      },
      "source": [
        "This looks much better. We have now learned how to delete missing values completely or impute them using background knowledge.\n",
        "\n",
        "After we have dealt with the missing records, I think we are ready to cluster. We will use k-means without actually referring to either age nor gender. Sorry! But it is good that you have learned how to deal with missing values, and we will use this knowledge later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0976aeab-0c7e-4e2e-92b7-4edf79e3f956",
      "metadata": {
        "id": "0976aeab-0c7e-4e2e-92b7-4edf79e3f956"
      },
      "source": [
        "## K-means\n",
        "\n",
        "To explore unsupervised machine learning, we look at the basics of clustering that delivers you powerful results very fast. In particular, we will use the k-means algorithm, which was invented by MacQueen in the late 1960s (https://en.wikipedia.org/wiki/K-means_clustering). Despite being ancient in terms of computer lives, k-means is still widely used, because it delivers good results with great computational performance. Computational performance in computing describes the effort we need in terms of computational resources. You will generally notice performance by the execution time.\n",
        "\n",
        "https://machinelearningmastery.com/clustering-algorithms-with-python/ provides a good overview of clustering algorithms that are implemented in Python.\n",
        "\n",
        "k-means tries to develop clusters by (1) initialising a pre-defined number (k) of randomly chosen centroids in the feature space. Centroids are simply the centre points of clusters. (2) The algorithm assigns each observation to the cluster with the closest centroid. (3) Based on how balanced this assignment is, the centroids are recalculated, and steps 1 and 2 are repeated until the algorithm balances out.\n",
        "\n",
        "\n",
        "Let's move on to some actual work with `teens`.\n",
        "\n",
        "Please create an `interests` dataframe by selecting columns 4 to 40 from `teens` with `interests = teens.iloc[:,4:40]`. Also, print out the first few rows of `interests`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d9f7e6",
      "metadata": {
        "strip.white": true,
        "id": "b5d9f7e6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "549dbc83",
      "metadata": {
        "id": "549dbc83"
      },
      "source": [
        "Since k-means is based on calculating distances between data points and their centroids, it will be strongly influenced by the magnitudes of the variables we cluster. Think about if for a moment! Just imagine one column having values running from 1 to 10 and another from 1 to 1000. How could we compare distances between them?\n",
        "\n",
        "k-means is thus very sensitive to input of varying size, length, etc. We have interests of very different ranges, as they are simply based on how many times a keyword appears in teenagers' social networking contributions.\n",
        "\n",
        "We therefore need to scale the values again so that they all fall into the same range. You remember this from supervised-machine learning? Then, we did it manually. Now, we make use of Python packages.\n",
        "\n",
        "Run `from scipy.stats import zscore` to import zscore, which is very popular in data analysis for standardisation: https://www.statisticshowto.com/probability-and-statistics/z-score/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b23252",
      "metadata": {
        "strip.white": true,
        "id": "44b23252"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e030b52c",
      "metadata": {
        "id": "e030b52c"
      },
      "source": [
        "Apply `zscore` and assign the results to a new dataframe interests_z with `interests_z = interests.apply(zscore)`. Finally, print out the first few columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18de4323",
      "metadata": {
        "scrolled": true,
        "id": "18de4323"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "10823cab-a10c-403a-86db-f2720786614c",
      "metadata": {
        "id": "10823cab-a10c-403a-86db-f2720786614c"
      },
      "source": [
        "It's clearly normalized around the means of the various columns - by the distance of the standard deviation.\n",
        "\n",
        "Let's import KMeans from `sklearn.cluster`. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37051771",
      "metadata": {
        "id": "37051771"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d9ea50",
      "metadata": {
        "id": "94d9ea50"
      },
      "source": [
        "Many of the decisions in analytics are more an art than a science. We need to often estimate many parameters – either based on previous experience or using background knowledge. K-means is famous for heavily depending on `k` or the number of clusters we want to assign. We need to tell Python which `k` to use. We explore this further in the workshop.\n",
        "\n",
        "We decide 5 clusters is enough and assign `k = 5`. Run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06026372",
      "metadata": {
        "strip.white": true,
        "id": "06026372"
      },
      "outputs": [],
      "source": [
        "#Keep cell\n",
        "\n",
        "k = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a709b501",
      "metadata": {
        "id": "a709b501"
      },
      "source": [
        "Now we are ready to cluster. Fit `interests_z` and assign it to `teen_clusters` the way you know. It is just a copy and paste job from before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8eba38",
      "metadata": {
        "strip.white": true,
        "id": "0b8eba38"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f7f5d04d",
      "metadata": {
        "id": "f7f5d04d"
      },
      "source": [
        "Let's investigate the size of the clusters with `.labels_` and `np.bincount`. Run `np.bincount(teen_clusters.labels_)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76838b2a",
      "metadata": {
        "strip.white": true,
        "id": "76838b2a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4e0068cb",
      "metadata": {
        "id": "4e0068cb"
      },
      "source": [
        "I have noticed very different results depending on the kmeans results. I suggest to rerun kmeans a couple of times until you see a distribution that looks ok. You want to especially avoid clusters of only one 1 item but we are just practicing here.\n",
        "\n",
        "We can also look at the centroids/centres with `teen_clusters.cluster_centers_`. You can can pretty-print this in a data frame. Run:\n",
        "```\n",
        "interests_centroids = pd.DataFrame(teen_clusters.cluster_centers_, columns=interests_z.columns)\n",
        "interests_centroids\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1125f3a2",
      "metadata": {
        "strip.white": true,
        "id": "1125f3a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dce294b2",
      "metadata": {
        "id": "dce294b2"
      },
      "source": [
        "Each row corresponds to a cluster, and each column corresponds to a feature. So, the values in `interests_centroids` represent the average value of each feature for the samples in the respective cluster. The maximum average value is the most likely cluster they belong to.\n",
        "\n",
        "A simple way to detect clusters is thus to find the maximum values in the columns. Try it by using the `idxmax()` function from Pandas: `interests_centroids.idxmax()`. Check out its documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a1c371",
      "metadata": {
        "id": "c3a1c371"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "aba762a2",
      "metadata": {
        "id": "aba762a2"
      },
      "source": [
        "Hopefully, your results look similar to the table from Lantz (2013) on p. 288:\n",
        "\n",
        "![alt text](https://github.com/goto4711/cdai/blob/main/teen-clusters.png?raw=true \"\")\n",
        "\n",
        "Do the names of the clusters make sense to you? Do you remember all those teenage movies you watched?\n",
        "\n",
        "Next, let's continue with another type of analysis and do some further reasoning on the data with the new insights.\n",
        "\n",
        "## Analyse Results\n",
        "\n",
        "Let's first assign each teen to a cluster. Please, add a column called 'cluster' to the teen dataframe with `teens['cluster'] = np.array(teen_clusters.labels_)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb8bb71",
      "metadata": {
        "strip.white": true,
        "id": "cbb8bb71"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e1ffc786",
      "metadata": {
        "id": "e1ffc786"
      },
      "source": [
        "Let's take a look at `teens` with `head()`. All the way to the right, you can find the cluster assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438cc319",
      "metadata": {
        "strip.white": true,
        "id": "438cc319"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "551869f3",
      "metadata": {
        "id": "551869f3"
      },
      "source": [
        "Let's print out the first 5 teens and only the columns 'cluster', 'gender', 'age' and 'friends'. I hope you remember how this works. How do we select the first 5 rows? How do we select the columns?\n",
        "\n",
        "Tip: `teens.loc` is what you are looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad202adb",
      "metadata": {
        "strip.white": true,
        "id": "ad202adb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "27d2949e",
      "metadata": {
        "id": "27d2949e"
      },
      "source": [
        "Since we have learned earlier how to group by particular interests, let's aggregate the teens' features using the clusters.\n",
        "\n",
        "Print out the average ages per cluster. Do you remember how this works? Tip: Replace `gradyear` in the earlier `group_by` statements with `cluster` and you are ready to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28896fdd",
      "metadata": {
        "strip.white": true,
        "id": "28896fdd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0c65cc9b",
      "metadata": {
        "id": "0c65cc9b"
      },
      "source": [
        "The clusters do not differ in terms of ages very much. There is no immediate relation between age and interest clusters. Now, let's look at the female contribution to each cluster. How? Which column to you have to use instead of age?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0275ee03",
      "metadata": {
        "strip.white": true,
        "id": "0275ee03"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9175f7b0",
      "metadata": {
        "id": "9175f7b0"
      },
      "source": [
        "Overall, 74 per cent of the SNS's users are female. That's why they contribute so much to each cluster. Can you see the cluster that has the most female users? Do you know why? Look back to the earlier analysis of the interests linked to the clusters ...\n",
        "\n",
        "But please note that we set earlier female to 1 or 0 only where gender is not NaN. So, for 'no_gender' individuals, `teens['female']` will be NaN. `groupby().mean()` will skip these NaNs by default. This is important for the interpretation, as the mean is now only the proportion of females among those whose gender is known in that cluster.\n",
        "\n",
        "You can check for the average number of friends per cluster now. Just define the target of the aggregation per cluster as friends instead of female or age in the expressions above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e434722",
      "metadata": {
        "strip.white": true,
        "id": "1e434722"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "aeab523b",
      "metadata": {
        "id": "aeab523b"
      },
      "source": [
        "Here, the differences are stronger. We suspect that the number of friends played a key role in assigning the clusters. That's the nature of a social network, I guess.\n",
        "\n",
        "Let's finally ask Gemini about the advantages and drawbacks of k-means and possible alternatives with examples."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wFp8axZK8fmQ"
      },
      "id": "wFp8axZK8fmQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d26df126",
      "metadata": {
        "id": "d26df126"
      },
      "source": [
        "In the workshop, we will simply go one more time through another example of clustering and supervised machine learning. There are always the same steps involved in data science. These steps are the main lesson of today:\n",
        "\n",
        "1. Get data\n",
        "1. Clean data and prepare it for the machine-learning algorithm you have chosen\n",
        "1. Run machine learning\n",
        "1. Evaluate, analyse and report\n",
        "\n",
        "Step 2 is by far the most difficult one, as the machine learning is easier with modern packages like sklearn. But it is also always hard to analyse the results."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "tags,message,strip.white,-all",
      "main_language": "R",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}